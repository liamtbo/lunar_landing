{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will require the following dependencies and should be run in python 3.11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# pip install gymnasium\n",
    "# pip install torch\n",
    "# pip install tqdm\n",
    "# pip install numpy\n",
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a solution to the lunar-landing problem, I chose to implement a Deep Q-Network (DQN). Deep Q-Networks have become famous for their significant advancements within reinforcement learning and for achieving human-level performance on Atari games.\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "My DQN consists of an input layer for observations of dimension 8, one hidden layer with 128 neurons, and an action output layer with dimension 4. In order to account for the continuous observation space, Iâ€™ll be using a neural network to represent state-action values. In my model, both the input layer and the single hidden layer are set with ReLU activation functions to introduce non-linearity to the model. This non-linearity will help the model map a complex non-linear function with the purpose of landing our lunar-landing vehicle safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module): \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReplayBuffer class will hold our off-policy samples derived from interaction with the environment and will be used for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"terminated\"))\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, replay_buff_capacity):\n",
    "        self.ReplayBuffer = deque([], maxlen=replay_buff_capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.ReplayBuffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.ReplayBuffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ReplayBuffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot_rewards function will continiously plot our sum reward allowing us to visualize the network improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('reward')\n",
    "    plt.plot(rewards_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My optimize_network function performs gradient descent on the loss. In my code, I use the Adam Optimizer due to its adaptive learning rates and momentum estimation.\n",
    "\n",
    "The Adam optimization equations are:\n",
    "$$\n",
    "\\mathbf{m}_t = \\beta_m \\mathbf{m}_{t-1} + (1 - \\beta_m) g_t \\\\\n",
    "\\mathbf{v}_t = \\beta_v \\mathbf{v}_{t-1} + (1 - \\beta_v) g_t^2\n",
    "$$\n",
    "$$\n",
    "\\hat{\\mathbf{m}}_t = \\frac{\\mathbf{m}_t}{1 - \\beta_m^t} \\quad \\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_v^t}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w}_t = \\mathbf{w}_{t-1} + \\frac{\\alpha \\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon}\n",
    "$$\n",
    "Momentum is a technique to accelerate gradient descent by considering past gradients to smooth out the updates. It helps avoid local minima and can speed up convergence. Adaptive learning rate methods adjust the learning rate dynamically to improve convergence.\n",
    "\n",
    "To perform the Q-learning update, I use a target network and a policy network to help stabilize training and improve the convergence of the algorithm.\n",
    "\n",
    "The loss function used here is the Huber loss, which combines advantages from both Mean Squared Error (MSE) and Mean Absolute Error (MAE) by being less sensitive to outliers than MSE and more stable than MAE.\n",
    "\n",
    "$$\n",
    "L_\\delta(y, \\hat{y}) =\n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "\\delta \\cdot (|y - \\hat{y}| - \\frac{1}{2} \\delta) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "where:\n",
    "- y is the true value,\n",
    "- y^ is the predicted value, and\n",
    "- delta is a threshold parameter that determines the point where the loss function changes from quadratic to linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(functions, hp, replay: ReplayBuffer):\n",
    "    if len(replay) < hp[\"minibatch_size\"]:\n",
    "        return\n",
    "    device = functions[\"device\"]\n",
    "    policy_nn = functions[\"policy_nn\"]\n",
    "    target_nn = functions[\"target_nn\"]\n",
    "    loss_function = functions[\"loss_function\"]\n",
    "    optimizer = functions[\"optimizer\"]\n",
    "\n",
    "    batch = replay.sample(hp[\"minibatch_size\"])\n",
    "    batch = Transition(*zip(*batch))\n",
    "\n",
    "    states = torch.tensor(np.array(batch.state), dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(batch.action, dtype=torch.long, device=device).unsqueeze(1)\n",
    "    rewards = torch.tensor(batch.reward, dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor(np.array(batch.next_state), dtype=torch.float32, device=device)\n",
    "    terminated = torch.tensor(batch.terminated, dtype=int, device=device)\n",
    "\n",
    "    predicted  = policy_nn(states).gather(1, actions).squeeze(1)\n",
    "    with torch.no_grad():\n",
    "        next_states_qvalues = torch.max(target_nn(next_states), dim=1).values * (1 - terminated)\n",
    "    target = rewards + 0.99 * next_states_qvalues\n",
    "    loss = loss_function(predicted, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_nn.parameters(), 1)\n",
    "\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've written code for two different action selection functions. Decaying Epsilon-Greedy seemed to work better across the board then undecaying softmax which is why we'll use it in the final run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def e_greedy(q_values, functions, hp):\n",
    "    env = functions[\"env\"]\n",
    "    device = functions[\"device\"]\n",
    "    eps_threshold = hp[\"eps_end\"] + (hp[\"eps_start\"] - hp[\"eps_end\"]) \\\n",
    "                    * math.exp(-1. * hp[\"steps_done\"] / hp[\"eps_decay\"])\n",
    "    hp[\"steps_done\"] += 1\n",
    "    sample = torch.rand(1).item()\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return q_values.max(0).indices.item()\n",
    "    else:\n",
    "        action = torch.randint(0, env.action_space.n, size=(1,)).item()\n",
    "        return torch.tensor([action], device=device, dtype=torch.long).item()\n",
    "\n",
    "def softmax(state_qvalues, functions, hp):\n",
    "    state_qvalues_probabilities = torch.softmax(state_qvalues, dim=0)\n",
    "    state_qvalues_dis = torch.distributions.Categorical(state_qvalues_probabilities)\n",
    "    action = state_qvalues_dis.sample().item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where our policy neural network is tested and trained. After continuously interacting with the environment, the replay buffer will be large enough to start gradient descent. After the policy network is optimized, I perform a soft update on the target network. Soft updating provides stability by preventing the target network from shifting too rapidly and smooths convergence by reducing the risk of overfitting to recent experiences or changes in the policy network.\n",
    "$$\n",
    "\\theta_{\\text{target}} \\leftarrow \\tau \\theta_{\\text{policy}} + (1 - \\tau) \\theta_{\\text{target}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(functions, hp):\n",
    "    env = functions[\"env\"]\n",
    "    policy_nn = functions[\"policy_nn\"]\n",
    "    target_nn = functions[\"target_nn\"]\n",
    "    device = functions[\"device\"]\n",
    "    select_action = functions[\"select_action\"]\n",
    "    replay = ReplayBuffer(hp[\"ReplayBuffer_capacity\"])\n",
    "    reward_sum = 0\n",
    "\n",
    "    for episode in range(hp[\"episodes\"]):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        terminated = False\n",
    "\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                state_qvalues = policy_nn(state)\n",
    "            action = select_action(state_qvalues, functions, hp)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            replay.push(state.tolist(), action, reward, next_state, terminated)\n",
    "            state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "\n",
    "            for _ in range(hp[\"replay_steps\"]):\n",
    "                optimize_network(functions, hp, replay)\n",
    "\n",
    "            # soft update\n",
    "            TAU = 0.005\n",
    "            target_net_state_dict = target_nn.state_dict()\n",
    "            policy_net_state_dict = policy_nn.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_nn.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "            if terminated or truncated:\n",
    "                episode_rewards.append(reward_sum) # TODO\n",
    "                reward_sum = 0\n",
    "                plot_rewards()\n",
    "                break\n",
    "            \n",
    "    print('Complete')\n",
    "    plot_rewards(show_result=True)\n",
    "    plt.ioff()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main() is where we finally populate the hyperparameters, choose our functions, and run our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # env = gym.make('LunarLander-v2', render_mode=\"human\")\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    policy_nn = DQN(state_dim, action_dim).to(device)\n",
    "    target_nn = DQN(state_dim, action_dim).to(device)\n",
    "    target_nn.load_state_dict(policy_nn.state_dict())\n",
    "    lr = 1e-4\n",
    "\n",
    "    functions = {\n",
    "        \"env\": env,\n",
    "        \"policy_nn\": policy_nn,\n",
    "        \"target_nn\": target_nn,\n",
    "        \"loss_function\": nn.SmoothL1Loss(),\n",
    "        \"optimizer\": optim.Adam(policy_nn.parameters(), lr=lr),\n",
    "        \"device\": device,\n",
    "        \"select_action\": e_greedy # softmax or e_greedy\n",
    "    }\n",
    "    hp = {\n",
    "        \"episodes\": 500,\n",
    "        \"graph_increment\": 10,\n",
    "        \"replay_steps\": 2,\n",
    "        \"learning_rate\": lr,\n",
    "        \"tau\": 0.001,\n",
    "        \"ReplayBuffer_capacity\": 10000,\n",
    "        \"minibatch_size\": 64,\n",
    "        \"eps_end\": 0.05,\n",
    "        \"eps_start\": 0.9,\n",
    "        \"eps_decay\": 500,\n",
    "        \"steps_done\": 0,\n",
    "    }\n",
    "\n",
    "    training_loop(functions, hp)\n",
    "    # torch.save(policy_nn.state_dict(), \"learned_policy.pth\")\n",
    "    env.close()\n",
    "    return policy_nn\n",
    "\n",
    "policy_nn = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our network for 1000 episodes, it's converged to a solution and is ready to be tested below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"human\")\n",
    "for episode in range(5):\n",
    "    state, _ = env.reset()\n",
    "    for t in count():\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        state_qvalues = policy_nn(state)\n",
    "        action = state_qvalues.max(0).indices.item()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
